# Avdanced Data Analysis - Fall 2025 - Capstone Project - Maxime Cherix

This repository contains the source code and replication materials for the research project: **"How do physical climate exposure and the quality of climate risk disclosure jointly affect market-perceived risk?"**

The project constructs a Physical Risk Score (PRS) and uses Natural Language Processing (ClimateBERT) to derive a Disclosure Quality Score (DQS) from 10-K filings, analyzing their interactive effect on idiosyncratic stock return volatility.

## Repository Structure

The project is organized as follows:

├── data/
│   ├── raw/            # Place downloaded data here (see instructions below)
│   └── processed/      # Intermediate and final datasets generated by the code
├── src/
│   ├── 01_feature_construction.ipynb       # Financial controls & PRS construction
│   ├── 02_10k_nlp_analysis.ipynb           # NLP pipeline (ClimateBERT) for DQS
│   ├── 03_model_training.ipynb             # ML models (Random Forest, Gradient Boosting)
│   ├── 04_presentation_plots.ipynb         # SHAP plots and result visualizations
├── .gitignore
├── README.md
└── requirements.txt    # Python dependencies

Prerequisites
To run this code, you need:
	•	Python 3.8+
	•	Jupyter Notebook or JupyterLab
	•	Standard data science libraries (pandas, numpy, scikit-learn, matplotlib)
	•	NLP libraries (transformers, torch)
Installation & Setup
1. Clone the Repository
Bash

git clone https://github.com/maximecherix/climate-disclosure-nlp
cd YOUR_REPO_NAME

2. Set up a Virtual Environment (Recommended)
It is highly recommended to use a virtual environment to manage dependencies.
Using venv:
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate

Using conda:
conda create --name climate_risk python=3.9
conda activate climate_risk

3. Install Dependencies
pip install -r requirements.txt

Data Access
Due to file size limits on GitHub, the raw data (including 10-K filings and disaster datasets) is hosted externally on Google Drive.
CLICK ON THIS LINK TO DOWNLOAD RAW DATA: https://drive.google.com/drive/folders/1jYqWfovVHXVcZ4hfORi0wXH5p6KYu357?usp=sharing
Instructions:
	1	Download the .zip file and .csv files from the link above.
	2	Extract the contents.
	3	Move the files into the data/raw/ folder in this repository.
Your directory should look like this after downloading:
Plaintext

data/
└── raw/
    ├── compustat_financials.csv
    ├── public_emdat_disasters.xlsx
    ├── 10k_filings/
    │   ├── firm_A_2020.txt
    │   └── ...
    └── ...

Running the Analysis
The notebooks are numbered sequentially to ensure data flows correctly from raw inputs to final results. Please run them in the following order:
	1	01_feature_construction.ipynb
	◦	Input: Raw financial and disaster data, relevance and specificity scores (from Notebook 02).
	◦	Output: Target volatility, cleaned financial controls, Physical Risk Score (PRS), Disclosure Quality Score.
	2	02_10k_nlp_analysis.ipynb
	◦	Input: Raw 10-K text files.
	◦	Process: Runs ClimateBERT to extract 
	◦	Output: Relevance and specificity scores.
	◦	Note: This notebook requires significant computational resources (GPU recommended).
	3	03_model_training.ipynb
	◦	Input: Merged dataset (Financials + PRS + DQS).
	◦	Process: Trains ElasticNet, Random Forest, and Gradient Boosting models.
	◦	Output: Trained model objects and performance metrics.
	4	04_presentation_plots.ipynb
	◦	Process: Generates SHAP summary plots, interaction plots, and regression tables.
	◦	Output: Figures used in the final paper.

Computational Note
The NLP analysis in notebook 02 uses transformer-based models.
	•	Runtime: On a standard CPU, processing the full sample of 10-Ks may take several days.
	•	Optimization: The code is set to run on cuda (GPU) if available, otherwise it defaults to CPU.

Author
Maxime  HEC Lausanne maxime.cherix@unil.ch

