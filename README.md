# Advanced Data Analysis - Fall 2025 - Capstone Project

This repository contains the source code and replication materials for the research project: **"How do physical climate exposure and the quality of climate risk disclosure jointly affect market-perceived risk?"**

The project constructs a Physical Risk Score (PRS) and uses Natural Language Processing (ClimateBERT) to derive a Disclosure Quality Score (DQS) from 10-K filings, analyzing their interactive effect on idiosyncratic stock return volatility.

## Repository Structure

The project is organized as follows:
```bash
├── data/
│   ├── raw/            # Place downloaded data here (see instructions below)
│   └── processed/      # Intermediate and final datasets generated by the code
├── src/
│   ├── 01_feature_construction.ipynb       # Financial controls & PRS construction
│   ├── 02_10k_nlp_analysis.ipynb           # NLP pipeline (ClimateBERT) for DQS
│   ├── 03_model_training.ipynb             # ML models (Random Forest, Gradient Boosting)
│   ├── 04_presentation_plots.ipynb         # SHAP plots and result visualizations
├── .gitignore
├── README.md
└── requirements.txt    # Python dependencies
```
## Prerequisites
To run this code, you need:
- Python 3.8+
- Jupyter Notebook or JupyterLab
- Standard data science libraries (pandas, numpy, scikit-learn, matplotlib)
- NLP libraries (transformers, torch)
Installation & Setup
1. Clone the Repository
```bash

git clone https://github.com/maximecherix/climate-disclosure-nlp
cd climate-disclosure-nlp
```
2. Set up a Virtual Environment (Recommended)
It is highly recommended to use a virtual environment to manage dependencies.
Using venv:
```bash
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate
```
Using conda:
```bash
conda create --name climate_risk python=3.9
conda activate climate_risk
```
3. Install Dependencies
```bash
pip install -r requirements.txt
```
## Data Access
Due to file size limits on GitHub, the raw data (including 10-K filings and disaster datasets) is hosted externally on Google Drive.

### Instructions
1. Download the .zip file and .csv files from the following Google Drive:
https://drive.google.com/drive/folders/1jYqWfovVHXVcZ4hfORi0wXH5p6KYu357?usp=sharing
2. Extract the contents.
3. Move the files into the data/raw/ folder in this repository.
Your directory should look like this after downloading:
```bash
data/
└── raw/
    ├── compustat_financials.csv
    ├── public_emdat_disasters.xlsx
    ├── 10k_filings/
    │   ├── firm_A_2020.txt
    │   └── ...
    └── ...
```
## Running the Analysis
The notebooks are numbered sequentially to ensure data flows correctly from raw inputs to final results. Please run them in the following order:

### 1. `01_feature_construction.ipynb`

- **Input:**  
  Raw financial and disaster data, relevance and specificity scores (from Notebook 02)
- **Output:**  
  - Target volatility  
  - Cleaned financial controls  
  - Physical Risk Score (PRS)  
  - Disclosure Quality Score (DQS)

### 2. `02_10k_nlp_analysis.ipynb`

- **Input:**  
  Raw 10-K text files
- **Process:**  
  Runs ClimateBERT to extract disclosure metrics
- **Output:**  
  Relevance and specificity scores
- **Note:**  
  ⚠️ This notebook requires significant computational resources (GPU recommended)

### 3. `03_model_training.ipynb`

- **Input:**  
  Merged dataset (Financials + PRS + DQS)
- **Process:**  
  Trains ElasticNet, Random Forest, and Gradient Boosting models
- **Output:**  
  Trained model objects and performance metrics

### 4. `04_presentation_plots.ipynb`

- **Process:**  
  Generates SHAP summary plots, interaction plots, and regression tables
- **Output:**  
  Figures used in the final paper

## Computational Note
The NLP analysis in notebook 02 uses transformer-based models.
- Runtime: On a standard CPU, processing the full sample of 10-Ks may take several days.
- Optimization: The code is set to run on cuda (GPU) if available, otherwise it defaults to CPU.

## Author
Maxime Cherix, HEC Lausanne, maxime.cherix@unil.ch
